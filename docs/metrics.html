---

title: Title


keywords: fastai
sidebar: home_sidebar



nb_path: "metrics.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: metrics.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="sklm_to_scorer" class="doc_header"><code>sklm_to_scorer</code><a href="https://github.com/marcossantanaioc/mlmetrics/tree/master/mlmetrics/metrics.py#L12" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>sklm_to_scorer</code>(<strong><code>func</code></strong>, <strong><code>transform</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Converts func into sklearn scorer</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="R2Score" class="doc_header"><code>R2Score</code><a href="https://github.com/marcossantanaioc/mlmetrics/tree/master/mlmetrics/metrics.py#L18" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>R2Score</code>(<strong><code>y_true</code></strong>=<em><code>None</code></em>, <strong><code>y_pred</code></strong>=<em><code>None</code></em>, <strong><code>sample_weight</code></strong>=<em><code>None</code></em>, <strong><code>multioutput</code></strong>:<code>str</code>=<em><code>'uniform_average'</code></em>, <strong><code>transform</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>:math:<code>R^2</code> (coefficient of determination) regression score function.</p>
<p>Arguments</p>
<p>y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.</p>
<p>y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.</p>
<p>sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.</p>
<p>multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'
    Defines aggregating of multiple output scores.
    Array-like value defines weights used to average scores.
    Default is "uniform_average".
    'raw_values' :
        Returns a full set of scores in case of multioutput input.
    'uniform_average' :
        Scores of all outputs are averaged with uniform weight.
    'variance_weighted' :
        Scores of all outputs are averaged, weighted by the variances
        of each individual output.
    .. versionchanged:: 0.19
        Default value of multioutput is 'uniform_average'.</p>
<p>Returns:
    r2 : float
        The  score :math:<code>R^2</code> or ndarray of scores if ‘multioutput’ is ‘raw_values’.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MSEScore" class="doc_header"><code>MSEScore</code><a href="https://github.com/marcossantanaioc/mlmetrics/tree/master/mlmetrics/metrics.py#L60" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MSEScore</code>(<strong><code>y_true</code></strong>=<em><code>None</code></em>, <strong><code>y_pred</code></strong>=<em><code>None</code></em>, <strong><code>sample_weight</code></strong>=<em><code>None</code></em>, <strong><code>multioutput</code></strong>:<code>str</code>=<em><code>'uniform_average'</code></em>, <strong><code>squared</code></strong>=<em><code>True</code></em>, <strong><code>transform</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Mean squared error regression loss.</p>
<p>Arguments</p>
<p>y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.</p>
<p>y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.</p>
<p>sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.</p>
<p>multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.
    'raw_values' :
        Returns a full set of errors in case of multioutput input.
    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.</p>
<p>Returns:
    mse : float
         If True returns MSE value, if False returns RMSE value.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MAEScore" class="doc_header"><code>MAEScore</code><a href="https://github.com/marcossantanaioc/mlmetrics/tree/master/mlmetrics/metrics.py#L95" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MAEScore</code>(<strong><code>y_true</code></strong>=<em><code>None</code></em>, <strong><code>y_pred</code></strong>=<em><code>None</code></em>, <strong><code>sample_weight</code></strong>=<em><code>None</code></em>, <strong><code>multioutput</code></strong>:<code>str</code>=<em><code>'uniform_average'</code></em>, <strong><code>transform</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Mean absolute error regression loss.</p>
<p>Arguments</p>
<p>y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Ground truth (correct) target values.</p>
<p>y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Estimated target values.</p>
<p>sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.</p>
<p>multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'
    Defines aggregating of multiple output values.
    Array-like value defines weights used to average errors.
    'raw_values' :
        Returns a full set of errors in case of multioutput input.
    'uniform_average' :
        Errors of all outputs are averaged with uniform weight.</p>
<p>Returns:
    mae : float
         returns MAE.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="BalancedAccuracyScore" class="doc_header"><code>BalancedAccuracyScore</code><a href="https://github.com/marcossantanaioc/mlmetrics/tree/master/mlmetrics/metrics.py#L130" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>BalancedAccuracyScore</code>(<strong><code>y_true</code></strong>=<em><code>None</code></em>, <strong><code>y_pred</code></strong>=<em><code>None</code></em>, <strong><code>sample_weight</code></strong>=<em><code>None</code></em>, <strong><code>adjusted</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>transform</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Computes Matthew's correlation coefficient between actual and predicted values.
Arguments</p>

<pre><code>y_true : array-like
    Ground truth (correct) target values.

y_pred : array-like
    Estimated targets as returned by a classifier.

sample_weight : array-like, default=None
    Sample weights.

adjusted : bool, default=False
    When true, the result is adjusted for chance, so that random performance would score 0, while keeping perfect performance at a score of 1.


</code></pre>
<p>Returns:
    balanced_accuracy : float
        Balanced accuracy score.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MatthewsCorrCoef" class="doc_header"><code>MatthewsCorrCoef</code><a href="https://github.com/marcossantanaioc/mlmetrics/tree/master/mlmetrics/metrics.py#L159" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MatthewsCorrCoef</code>(<strong><code>y_true</code></strong>=<em><code>None</code></em>, <strong><code>y_pred</code></strong>=<em><code>None</code></em>, <strong><code>sample_weight</code></strong>=<em><code>None</code></em>, <strong><code>transform</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Computes Matthew's correlation coefficient between actual and predicted values.
Arguments</p>

<pre><code>y_true : array-like
    Ground truth (correct) target values.

y_pred : array-like
    Estimated targets as returned by a classifier.

sample_weight : array-like
    Sample weights.


</code></pre>
<p>Returns:
    mcc : float
    The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction).</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="PrecisionScore" class="doc_header"><code>PrecisionScore</code><a href="https://github.com/marcossantanaioc/mlmetrics/tree/master/mlmetrics/metrics.py#L185" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>PrecisionScore</code>(<strong><code>y_true</code></strong>=<em><code>None</code></em>, <strong><code>y_pred</code></strong>=<em><code>None</code></em>, <strong><code>average</code></strong>:<code>str</code>=<em><code>'binary'</code></em>, <strong><code>pos_label</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>sample_weight</code></strong>=<em><code>None</code></em>, <strong><code>transform</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Computes Precision between actual and predicted values
Arguments</p>

<pre><code>y_true : array-like
    Ground truth (correct) target values.

y_pred : array-like
    Estimated targets as returned by a classifier.


average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, default='binary'

    This parameter is required for multiclass/multilabel targets.
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:
    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like
    Sample weights.


</code></pre>
<p>Returns:
    precision : float (if average is not None) or array of float of shape (n_unique_labels,)
        Precision of the positive class in binary classification or weighted
        average of the precision of each class for the multiclass task.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="RecallScore" class="doc_header"><code>RecallScore</code><a href="https://github.com/marcossantanaioc/mlmetrics/tree/master/mlmetrics/metrics.py#L238" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>RecallScore</code>(<strong><code>y_true</code></strong>=<em><code>None</code></em>, <strong><code>y_pred</code></strong>=<em><code>None</code></em>, <strong><code>average</code></strong>:<code>str</code>=<em><code>'binary'</code></em>, <strong><code>pos_label</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>sample_weight</code></strong>=<em><code>None</code></em>, <strong><code>transform</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Computes Recall (Senstivity) between actual and predicted values
Arguments</p>

<pre><code>y_true : array-like
    Ground truth (correct) target values.

y_pred : array-like
    Estimated targets as returned by a classifier.


average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, default='binary'

    This parameter is required for multiclass/multilabel targets.
    If ``None``, the scores for each class are returned. Otherwise, this
    determines the type of averaging performed on the data:
    ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
    ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
    ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
    ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

sample_weight : array-like
    Sample weights.


</code></pre>
<p>Returns:
    recall : float (if average is not None) or array of float of shape (n_unique_labels,)
        Recall of the positive class in binary classification or weighted
        average of the recall of each class for the multiclass task.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="FBetaScore" class="doc_header"><code>FBetaScore</code><a href="https://github.com/marcossantanaioc/mlmetrics/tree/master/mlmetrics/metrics.py#L291" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>FBetaScore</code>(<strong><code>y_true</code></strong>=<em><code>None</code></em>, <strong><code>y_pred</code></strong>=<em><code>None</code></em>, <strong><code>beta</code></strong>:<code>float</code>=<em><code>0.5</code></em>, <strong><code>pos_label</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>average</code></strong>:<code>str</code>=<em><code>'binary'</code></em>, <strong><code>sample_weight</code></strong>=<em><code>None</code></em>, <strong><code>transform</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Compute the F-beta score between actual and predicted values</p>

<pre><code>Arguments

</code></pre>
<p>y_true : array-like
    Ground truth (correct) target values.</p>
<p>y_pred : array-like
    Estimated targets as returned by a classifier.</p>
<p>average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, default='binary'</p>

<pre><code>This parameter is required for multiclass/multilabel targets.
If ``None``, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:
``'binary'``:
Only report results for the class specified by ``pos_label``.
This is applicable only if targets (``y_{true,pred}``) are binary.
``'micro'``:
Calculate metrics globally by counting the total true positives,
false negatives and false positives.
``'macro'``:
Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.
``'weighted'``:
Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters 'macro' to account for label imbalance; it can result in an
F-score that is not between precision and recall.
``'samples'``:
Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
:func:`accuracy_score`).

</code></pre>
<p>sample_weight : array-like
    Sample weights.</p>
<p>Returns:
    fbeta_score : float (if average is not None) or array of float, shape = [n_unique_labels]
    F-beta score of the positive class in binary classification or weighted average of the F-beta score of each class for the multiclass task.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ROCAucScore" class="doc_header"><code>ROCAucScore</code><a href="https://github.com/marcossantanaioc/mlmetrics/tree/master/mlmetrics/metrics.py#L342" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ROCAucScore</code>(<strong><code>y_true</code></strong>=<em><code>None</code></em>, <strong><code>y_score</code></strong>=<em><code>None</code></em>, <strong><code>average</code></strong>:<code>str</code>=<em><code>'macro'</code></em>, <strong><code>sample_weight</code></strong>=<em><code>None</code></em>, <strong><code>transform</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.
Arguments</p>
<p>y_true : array-like
    Ground truth (correct) target values.</p>
<p>y_score : array-like</p>

<pre><code>Target scores.
* In the binary case, it corresponds to an array of shape
  `(n_samples,)`. Both probability estimates and non-thresholded
  decision values can be provided. The probability estimates correspond
  to the **probability of the class with the greater label**,
  i.e. `estimator.classes_[1]` and thus
  `estimator.predict_proba(X, y)[:, 1]`. The decision values
  corresponds to the output of `estimator.decision_function(X, y)`.
  See more information in the :ref:`User guide &lt;roc_auc_binary&gt;`;
* In the multiclass case, it corresponds to an array of shape
  `(n_samples, n_classes)` of probability estimates provided by the
  `predict_proba` method. The probability estimates **must**
  sum to 1 across the possible classes. In addition, the order of the
  class scores must correspond to the order of ``labels``,
  if provided, or else to the numerical or lexicographical order of
  the labels in ``y_true``. See more information in the
  :ref:`User guide &lt;roc_auc_multiclass&gt;`;
* In the multilabel case, it corresponds to an array of shape
  `(n_samples, n_classes)`. Probability estimates are provided by the
  `predict_proba` method and the non-thresholded decision values by
  the `decision_function` method. The probability estimates correspond
  to the **probability of the class with the greater label for each
  output** of the classifier. See more information in the
  :ref:`User guide &lt;roc_auc_multilabel&gt;`.


</code></pre>
<p>average : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'
    If <code>None</code>, the scores for each class are returned. Otherwise,
    this determines the type of averaging performed on the data:
    Note: multiclass ROC AUC currently only handles the 'macro' and
    'weighted' averages.
    <code>'micro'</code>:
        Calculate metrics globally by considering each element of the label
        indicator matrix as a label.
    <code>'macro'</code>:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    <code>'weighted'</code>:
        Calculate metrics for each label, and find their average, weighted
        by support (the number of true instances for each label).
    <code>'samples'</code>:
        Calculate metrics for each instance, and find their average.
    Will be ignored when <code>y_true</code> is binary.</p>
<p>sample_weight : array-like
    Sample weights.</p>
<p>Returns:
    auc : float
        Area under ROC for predicted classes</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="APScore" class="doc_header"><code>APScore</code><a href="https://github.com/marcossantanaioc/mlmetrics/tree/master/mlmetrics/metrics.py#L414" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>APScore</code>(<strong><code>y_true</code></strong>=<em><code>None</code></em>, <strong><code>y_score</code></strong>=<em><code>None</code></em>, <strong><code>average</code></strong>:<code>str</code>=<em><code>'macro'</code></em>, <strong><code>sample_weight</code></strong>=<em><code>None</code></em>, <strong><code>transform</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.
Arguments</p>
<p>y_true : array-like
    Ground truth (correct) target values.</p>
<p>y_score : array-like</p>

<pre><code>Target scores, can either be probability estimates of the positive
class, confidence values, or non-thresholded measure of decisions
(as returned by :term:`decision_function` on some classifiers).


</code></pre>
<p>average : {'micro', 'samples', 'weighted', 'macro'} or None,             default='macro'
    If <code>None</code>, the scores for each class are returned. Otherwise,
    this determines the type of averaging performed on the data:
    <code>'micro'</code>:
        Calculate metrics globally by considering each element of the label
        indicator matrix as a label.
    <code>'macro'</code>:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
    <code>'weighted'</code>:
        Calculate metrics for each label, and find their average, weighted
        by support (the number of true instances for each label).
    <code>'samples'</code>:
        Calculate metrics for each instance, and find their average.
    Will be ignored when <code>y_true</code> is binary.</p>
<p>pos_label : int or str, default=1
    The label of the positive class. Only applied to binary <code>y_true</code>.
    For multilabel-indicator <code>y_true</code>, <code>pos_label</code> is fixed to 1.</p>
<p>sample_weight : array-like of shape (n_samples,), default=None
    Sample weights.</p>
<p>Returns:
    auc : float
        Area under ROC for predicted classes</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">transform</span><span class="o">=</span><span class="kc">False</span>
<span class="n">iterators</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;regression&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;R2&#39;</span><span class="p">:</span> <span class="n">R2Score</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
                            <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span> <span class="n">MSEScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span><span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                            <span class="s1">&#39;MSE&#39;</span><span class="p">:</span> <span class="n">MSEScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span><span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                            <span class="s1">&#39;MAE&#39;</span><span class="p">:</span> <span class="n">MAEScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)},</span>
             <span class="s1">&#39;classification&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;MCC&#39;</span><span class="p">:</span> <span class="n">MatthewsCorrCoef</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
                                <span class="s1">&#39;Se&#39;</span><span class="p">:</span> <span class="n">RecallScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                                <span class="s1">&#39;Sp&#39;</span><span class="p">:</span> <span class="n">RecallScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                                <span class="s1">&#39;Precision&#39;</span><span class="p">:</span> <span class="n">PrecisionScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                                <span class="s1">&#39;PR-AUC&#39;</span><span class="p">:</span> <span class="n">APScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
                                <span class="s1">&#39;ROC-AUC&#39;</span><span class="p">:</span> <span class="n">ROCAucScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)},</span>
             <span class="s1">&#39;multi-class&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;MCC-Multi&#39;</span><span class="p">:</span> <span class="n">MatthewsCorrCoef</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
                             <span class="s1">&#39;Se-Multi&#39;</span><span class="p">:</span> <span class="n">RecallScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">),</span>
                             <span class="s1">&#39;Sp-Multi&#39;</span><span class="p">:</span> <span class="n">RecallScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">),</span>
                             <span class="s1">&#39;Precision-Multi&#39;</span><span class="p">:</span> <span class="n">PrecisionScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">),</span>
                             <span class="s1">&#39;PR-AUC-Multi&#39;</span><span class="p">:</span> <span class="n">APScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">),</span>
                             <span class="s1">&#39;ROC-AUC-Multi&#39;</span><span class="p">:</span> <span class="n">ROCAucScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)}}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_multi_reg</span><span class="p">,</span> <span class="n">y_multi_reg</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_targets</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">X_multi_class</span><span class="p">,</span> <span class="n">y_multi_class</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">y_multi_class_bin</span> <span class="o">=</span> <span class="n">label_binarize</span><span class="p">(</span><span class="n">y_multi_class</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_multi_class</span><span class="p">))</span>
<span class="n">X_multi_label</span><span class="p">,</span> <span class="n">y_multi_label</span> <span class="o">=</span> <span class="n">make_multilabel_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Multiregression</strong></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span>
<span class="n">Xtrain</span><span class="p">,</span><span class="n">Xtest</span><span class="p">,</span><span class="n">ytrain</span><span class="p">,</span><span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_multi_reg</span><span class="p">,</span> <span class="n">y_multi_reg</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>

<span class="n">iterators</span><span class="p">[</span><span class="s1">&#39;regression&#39;</span><span class="p">][</span><span class="s1">&#39;R2&#39;</span><span class="p">](</span><span class="n">ytest</span><span class="p">,</span> <span class="n">preds</span><span class="p">),</span> <span class="n">iterators</span><span class="p">[</span><span class="s1">&#39;regression&#39;</span><span class="p">][</span><span class="s1">&#39;RMSE&#39;</span><span class="p">](</span><span class="n">ytest</span><span class="p">,</span> <span class="n">preds</span><span class="p">),</span> <span class="n">iterators</span><span class="p">[</span><span class="s1">&#39;regression&#39;</span><span class="p">][</span><span class="s1">&#39;MSE&#39;</span><span class="p">](</span><span class="n">ytest</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span><span class="c1">#RegressorChain(RandomForestRegressor())</span>
<span class="n">cv_k</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_multi_reg</span><span class="p">,</span> <span class="n">y_multi_reg</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;MAE&#39;</span><span class="p">:</span><span class="n">MAEScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                                                                     <span class="s1">&#39;RMSE&#39;</span><span class="p">:</span><span class="n">MSEScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                                                                     <span class="s1">&#39;MSE&#39;</span><span class="p">:</span><span class="n">MSEScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                                                                     <span class="s1">&#39;R2&#39;</span><span class="p">:</span><span class="n">R2Score</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="kc">True</span><span class="p">)})</span>
<span class="n">cv_k</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Multiclass</strong></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">cv_k</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_multi_class</span><span class="p">,</span> <span class="n">y_multi_class</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;MCC&#39;</span><span class="p">:</span><span class="n">MatthewsCorrCoef</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                                                                         <span class="s1">&#39;Fbeta&#39;</span><span class="p">:</span><span class="n">FBetaScore</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)})</span>
<span class="n">cv_k</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Multilabel</strong></p>

</div>
</div>
</div>
</div>
 

