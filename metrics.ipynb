{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0713bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Marcos Santana\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.8\n",
      "IPython version      : 7.22.0\n",
      "\n",
      "sklearn : 1.0.2\n",
      "numpy   : 1.20.1\n",
      "fastcore: 1.3.29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext watermark\n",
    "%watermark -a 'Marcos Santana' -d -p sklearn,numpy,fastcore -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf51ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer, fbeta_score, f1_score, roc_auc_score, precision_score, recall_score, average_precision_score, balanced_accuracy_score, matthews_corrcoef\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05472d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sklm_to_scorer(func, transform:bool=False, **kwargs):\n",
    "    \"\"\"Converts func into sklearn scorer\"\"\"\n",
    "\n",
    "    return make_scorer(func, **kwargs)\n",
    "\n",
    "\n",
    "def R2Score(y_true=None, y_pred=None, sample_weight=None, multioutput:str='uniform_average', transform:bool=False, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    :math:`R^2` (coefficient of determination) regression score function.\n",
    "    \n",
    "    Arguments\n",
    "\n",
    "     y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Ground truth (correct) target values.\n",
    "        \n",
    "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Estimated target values.\n",
    "        \n",
    "    sample_weight : array-like of shape (n_samples,), default=None\n",
    "        Sample weights.\n",
    "        \n",
    "    multioutput : {'raw_values', 'uniform_average', 'variance_weighted'}, \\\n",
    "            array-like of shape (n_outputs,) or None, default='uniform_average'\n",
    "        Defines aggregating of multiple output scores.\n",
    "        Array-like value defines weights used to average scores.\n",
    "        Default is \"uniform_average\".\n",
    "        'raw_values' :\n",
    "            Returns a full set of scores in case of multioutput input.\n",
    "        'uniform_average' :\n",
    "            Scores of all outputs are averaged with uniform weight.\n",
    "        'variance_weighted' :\n",
    "            Scores of all outputs are averaged, weighted by the variances\n",
    "            of each individual output.\n",
    "        .. versionchanged:: 0.19\n",
    "            Default value of multioutput is 'uniform_average'.\n",
    "\n",
    "    Returns:\n",
    "        r2 : float\n",
    "            The  score :math:`R^2` or ndarray of scores if ‘multioutput’ is ‘raw_values’.\n",
    "    \"\"\"\n",
    "    if transform:\n",
    "        return sklm_to_scorer(r2_score, sample_weight=sample_weight, multioutput=multioutput, **kwargs)\n",
    "       \n",
    "    return partial(r2_score, sample_weight=sample_weight, multioutput=multioutput, **kwargs)#(y_true=y_true, y_pred=y_pred, sample_weight=sample_weight, multioutput=multioutput, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def MSEScore(y_true=None, y_pred=None, sample_weight=None, multioutput:str='uniform_average', squared=True, transform:bool=False, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Mean squared error regression loss.\n",
    "    \n",
    "    Arguments\n",
    "\n",
    "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Ground truth (correct) target values.\n",
    "        \n",
    "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Estimated target values.\n",
    "        \n",
    "    sample_weight : array-like of shape (n_samples,), default=None\n",
    "        Sample weights.\n",
    "        \n",
    "    multioutput : {'raw_values', 'uniform_average'} or array-like of shape \\\n",
    "            (n_outputs,), default='uniform_average'\n",
    "        Defines aggregating of multiple output values.\n",
    "        Array-like value defines weights used to average errors.\n",
    "        'raw_values' :\n",
    "            Returns a full set of errors in case of multioutput input.\n",
    "        'uniform_average' :\n",
    "            Errors of all outputs are averaged with uniform weight.\n",
    "\n",
    "    Returns:\n",
    "        mse : float\n",
    "             If True returns MSE value, if False returns RMSE value.\n",
    "    \"\"\"\n",
    "    if transform:\n",
    "        return sklm_to_scorer(mean_squared_error, sample_weight=sample_weight, multioutput=multioutput, squared=squared, **kwargs)\n",
    "       \n",
    "    return partial(mean_squared_error, sample_weight=sample_weight, multioutput=multioutput, squared=squared, **kwargs)\n",
    "\n",
    "\n",
    "def MAEScore(y_true=None, y_pred=None, sample_weight=None, multioutput:str='uniform_average', transform:bool=False, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Mean absolute error regression loss.\n",
    "    \n",
    "    Arguments\n",
    "\n",
    "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Ground truth (correct) target values.\n",
    "        \n",
    "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Estimated target values.\n",
    "        \n",
    "    sample_weight : array-like of shape (n_samples,), default=None\n",
    "        Sample weights.\n",
    "        \n",
    "    multioutput : {'raw_values', 'uniform_average'} or array-like of shape \\\n",
    "            (n_outputs,), default='uniform_average'\n",
    "        Defines aggregating of multiple output values.\n",
    "        Array-like value defines weights used to average errors.\n",
    "        'raw_values' :\n",
    "            Returns a full set of errors in case of multioutput input.\n",
    "        'uniform_average' :\n",
    "            Errors of all outputs are averaged with uniform weight.\n",
    "\n",
    "    Returns:\n",
    "        mae : float\n",
    "             returns MAE.\n",
    "    \"\"\"\n",
    "    if transform:\n",
    "        return sklm_to_scorer(mean_absolute_error, sample_weight=sample_weight, multioutput=multioutput, **kwargs)\n",
    "       \n",
    "    return partial(mean_absolute_error, sample_weight=sample_weight, multioutput=multioutput, **kwargs)\n",
    "\n",
    "\n",
    "def BalancedAccuracyScore(y_true=None, y_pred=None, sample_weight=None, adjusted:bool=False, transform:bool=False, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes Matthew's correlation coefficient between actual and predicted values.\n",
    "    Arguments\n",
    "\n",
    "        y_true : array-like\n",
    "            Ground truth (correct) target values.\n",
    "\n",
    "        y_pred : array-like\n",
    "            Estimated targets as returned by a classifier.\n",
    "\n",
    "        sample_weight : array-like, default=None\n",
    "            Sample weights. \n",
    "            \n",
    "        adjusted : bool, default=False\n",
    "            When true, the result is adjusted for chance, so that random performance would score 0, while keeping perfect performance at a score of 1.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        balanced_accuracy : float\n",
    "            Balanced accuracy score.\n",
    "    \"\"\"\n",
    "    if transform:\n",
    "        return sklm_to_scorer(balanced_accuracy_score, sample_weight=sample_weight, adjusted=adjusted, **kwargs)\n",
    "       \n",
    "    return partial(balanced_accuracy_score, sample_weight=sample_weight, adjusted=adjusted, **kwargs)\n",
    "\n",
    "\n",
    "def MatthewsCorrCoef(y_true=None, y_pred=None, sample_weight=None, transform:bool=False, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes Matthew's correlation coefficient between actual and predicted values.\n",
    "    Arguments\n",
    "\n",
    "        y_true : array-like\n",
    "            Ground truth (correct) target values.\n",
    "\n",
    "        y_pred : array-like\n",
    "            Estimated targets as returned by a classifier.\n",
    "\n",
    "        sample_weight : array-like\n",
    "            Sample weights.  \n",
    "\n",
    "\n",
    "    Returns:\n",
    "        mcc : float\n",
    "        The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction).\n",
    "    \"\"\"\n",
    "    if transform:\n",
    "        return sklm_to_scorer(matthews_corrcoef, sample_weight=sample_weight, **kwargs)\n",
    "       \n",
    "    return partial(matthews_corrcoef, sample_weight=sample_weight, **kwargs)\n",
    "    \n",
    "    \n",
    "def PrecisionScore(y_true=None, y_pred=None, average:str='binary', pos_label:int=1, sample_weight=None, transform:bool=False,**kwargs):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Computes Precision between actual and predicted values\n",
    "    Arguments\n",
    "\n",
    "        y_true : array-like\n",
    "            Ground truth (correct) target values.\n",
    "\n",
    "        y_pred : array-like\n",
    "            Estimated targets as returned by a classifier.\n",
    "\n",
    "\n",
    "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, default='binary'\n",
    "\n",
    "            This parameter is required for multiclass/multilabel targets.\n",
    "            If ``None``, the scores for each class are returned. Otherwise, this\n",
    "            determines the type of averaging performed on the data:\n",
    "            ``'binary'``:\n",
    "                Only report results for the class specified by ``pos_label``.\n",
    "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
    "            ``'micro'``:\n",
    "                Calculate metrics globally by counting the total true positives,\n",
    "                false negatives and false positives.\n",
    "            ``'macro'``:\n",
    "                Calculate metrics for each label, and find their unweighted\n",
    "                mean.  This does not take label imbalance into account.\n",
    "            ``'weighted'``:\n",
    "                Calculate metrics for each label, and find their average weighted\n",
    "                by support (the number of true instances for each label). This\n",
    "                alters 'macro' to account for label imbalance; it can result in an\n",
    "                F-score that is not between precision and recall.\n",
    "            ``'samples'``:\n",
    "                Calculate metrics for each instance, and find their average (only\n",
    "                meaningful for multilabel classification where this differs from\n",
    "                :func:`accuracy_score`).\n",
    "\n",
    "        sample_weight : array-like\n",
    "            Sample weights.  \n",
    "\n",
    "\n",
    "    Returns:\n",
    "        precision : float (if average is not None) or array of float of shape (n_unique_labels,)\n",
    "            Precision of the positive class in binary classification or weighted\n",
    "            average of the precision of each class for the multiclass task.\n",
    "    \"\"\"\n",
    "    if transform:\n",
    "        return sklm_to_scorer(precision_score, average=average, pos_label=pos_label, sample_weight=sample_weight, **kwargs)\n",
    "\n",
    "    return partial(precision_score, average=average, pos_label=pos_label, sample_weight=sample_weight, **kwargs)\n",
    "    \n",
    "    \n",
    "def RecallScore(y_true=None, y_pred=None, average:str='binary', pos_label:int=1, sample_weight=None,\n",
    "                transform:bool=False, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes Recall (Senstivity) between actual and predicted values\n",
    "    Arguments\n",
    "\n",
    "        y_true : array-like\n",
    "            Ground truth (correct) target values.\n",
    "\n",
    "        y_pred : array-like\n",
    "            Estimated targets as returned by a classifier.\n",
    "\n",
    "\n",
    "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, default='binary'\n",
    "\n",
    "            This parameter is required for multiclass/multilabel targets.\n",
    "            If ``None``, the scores for each class are returned. Otherwise, this\n",
    "            determines the type of averaging performed on the data:\n",
    "            ``'binary'``:\n",
    "                Only report results for the class specified by ``pos_label``.\n",
    "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
    "            ``'micro'``:\n",
    "                Calculate metrics globally by counting the total true positives,\n",
    "                false negatives and false positives.\n",
    "            ``'macro'``:\n",
    "                Calculate metrics for each label, and find their unweighted\n",
    "                mean.  This does not take label imbalance into account.\n",
    "            ``'weighted'``:\n",
    "                Calculate metrics for each label, and find their average weighted\n",
    "                by support (the number of true instances for each label). This\n",
    "                alters 'macro' to account for label imbalance; it can result in an\n",
    "                F-score that is not between precision and recall.\n",
    "            ``'samples'``:\n",
    "                Calculate metrics for each instance, and find their average (only\n",
    "                meaningful for multilabel classification where this differs from\n",
    "                :func:`accuracy_score`).\n",
    "\n",
    "        sample_weight : array-like\n",
    "            Sample weights.  \n",
    "\n",
    "\n",
    "    Returns:\n",
    "        recall : float (if average is not None) or array of float of shape (n_unique_labels,)\n",
    "            Recall of the positive class in binary classification or weighted\n",
    "            average of the recall of each class for the multiclass task.\n",
    "    \"\"\"\n",
    "\n",
    "    if transform:\n",
    "        return sklm_to_scorer(recall_score, average=average, pos_label=pos_label, sample_weight=sample_weight, **kwargs)\n",
    "\n",
    "    return partial(recall_score, average=average, pos_label=pos_label, sample_weight=sample_weight, **kwargs)\n",
    "\n",
    "def FBetaScore(y_true=None, y_pred=None, beta:float=0.5, pos_label:int=1, average:str='binary', sample_weight=None,transform:bool=False,**kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the F-beta score between actual and predicted values\n",
    "    \n",
    "        Arguments\n",
    "\n",
    "    y_true : array-like\n",
    "        Ground truth (correct) target values.\n",
    "\n",
    "    y_pred : array-like\n",
    "        Estimated targets as returned by a classifier.\n",
    "\n",
    "\n",
    "    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, default='binary'\n",
    "\n",
    "        This parameter is required for multiclass/multilabel targets.\n",
    "        If ``None``, the scores for each class are returned. Otherwise, this\n",
    "        determines the type of averaging performed on the data:\n",
    "        ``'binary'``:\n",
    "        Only report results for the class specified by ``pos_label``.\n",
    "        This is applicable only if targets (``y_{true,pred}``) are binary.\n",
    "        ``'micro'``:\n",
    "        Calculate metrics globally by counting the total true positives,\n",
    "        false negatives and false positives.\n",
    "        ``'macro'``:\n",
    "        Calculate metrics for each label, and find their unweighted\n",
    "        mean.  This does not take label imbalance into account.\n",
    "        ``'weighted'``:\n",
    "        Calculate metrics for each label, and find their average weighted\n",
    "        by support (the number of true instances for each label). This\n",
    "        alters 'macro' to account for label imbalance; it can result in an\n",
    "        F-score that is not between precision and recall.\n",
    "        ``'samples'``:\n",
    "        Calculate metrics for each instance, and find their average (only\n",
    "        meaningful for multilabel classification where this differs from\n",
    "        :func:`accuracy_score`).\n",
    "\n",
    "    sample_weight : array-like\n",
    "        Sample weights.  \n",
    "\n",
    "\n",
    "    Returns:\n",
    "        fbeta_score : float (if average is not None) or array of float, shape = [n_unique_labels]\n",
    "        F-beta score of the positive class in binary classification or weighted average of the F-beta score of each class for the multiclass task.\n",
    "    \"\"\"\n",
    "    if transform:\n",
    "        return sklm_to_scorer(fbeta_score, beta=beta, average=average, pos_label=pos_label, sample_weight=sample_weight, **kwargs)\n",
    "\n",
    "    return partial(fbeta_score, beta=beta, average=average, pos_label=pos_label, sample_weight=sample_weight, **kwargs)        \n",
    "\n",
    "def ROCAucScore(y_true=None, y_score=None, average:str='macro', sample_weight=None, transform:bool=False, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "    Arguments\n",
    "\n",
    "    y_true : array-like\n",
    "        Ground truth (correct) target values.\n",
    "\n",
    "\n",
    "    y_score : array-like\n",
    "\n",
    "        Target scores.\n",
    "        * In the binary case, it corresponds to an array of shape\n",
    "          `(n_samples,)`. Both probability estimates and non-thresholded\n",
    "          decision values can be provided. The probability estimates correspond\n",
    "          to the **probability of the class with the greater label**,\n",
    "          i.e. `estimator.classes_[1]` and thus\n",
    "          `estimator.predict_proba(X, y)[:, 1]`. The decision values\n",
    "          corresponds to the output of `estimator.decision_function(X, y)`.\n",
    "          See more information in the :ref:`User guide <roc_auc_binary>`;\n",
    "        * In the multiclass case, it corresponds to an array of shape\n",
    "          `(n_samples, n_classes)` of probability estimates provided by the\n",
    "          `predict_proba` method. The probability estimates **must**\n",
    "          sum to 1 across the possible classes. In addition, the order of the\n",
    "          class scores must correspond to the order of ``labels``,\n",
    "          if provided, or else to the numerical or lexicographical order of\n",
    "          the labels in ``y_true``. See more information in the\n",
    "          :ref:`User guide <roc_auc_multiclass>`;\n",
    "        * In the multilabel case, it corresponds to an array of shape\n",
    "          `(n_samples, n_classes)`. Probability estimates are provided by the\n",
    "          `predict_proba` method and the non-thresholded decision values by\n",
    "          the `decision_function` method. The probability estimates correspond\n",
    "          to the **probability of the class with the greater label for each\n",
    "          output** of the classifier. See more information in the\n",
    "          :ref:`User guide <roc_auc_multilabel>`.\n",
    "\n",
    "\n",
    "    average : {'micro', 'macro', 'samples', 'weighted'} or None, \\\n",
    "            default='macro'\n",
    "        If ``None``, the scores for each class are returned. Otherwise,\n",
    "        this determines the type of averaging performed on the data:\n",
    "        Note: multiclass ROC AUC currently only handles the 'macro' and\n",
    "        'weighted' averages.\n",
    "        ``'micro'``:\n",
    "            Calculate metrics globally by considering each element of the label\n",
    "            indicator matrix as a label.\n",
    "        ``'macro'``:\n",
    "            Calculate metrics for each label, and find their unweighted\n",
    "            mean.  This does not take label imbalance into account.\n",
    "        ``'weighted'``:\n",
    "            Calculate metrics for each label, and find their average, weighted\n",
    "            by support (the number of true instances for each label).\n",
    "        ``'samples'``:\n",
    "            Calculate metrics for each instance, and find their average.\n",
    "        Will be ignored when ``y_true`` is binary.\n",
    "\n",
    "    sample_weight : array-like\n",
    "        Sample weights.  \n",
    "\n",
    "\n",
    "    Returns:\n",
    "        auc : float \n",
    "            Area under ROC for predicted classes\n",
    "    \"\"\"\n",
    "    if transform:\n",
    "        return sklm_to_scorer(roc_auc_score, average=average, sample_weight=sample_weight, **kwargs)\n",
    "\n",
    "    return partial(roc_auc_score, average=average, sample_weight=sample_weight, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def APScore(y_true=None, y_score=None, average:str='macro', sample_weight=None, transform:bool=False, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "    Arguments\n",
    "\n",
    "    y_true : array-like\n",
    "        Ground truth (correct) target values.\n",
    "\n",
    "\n",
    "    y_score : array-like\n",
    "\n",
    "        Target scores, can either be probability estimates of the positive\n",
    "        class, confidence values, or non-thresholded measure of decisions\n",
    "        (as returned by :term:`decision_function` on some classifiers).\n",
    "\n",
    "\n",
    "    average : {'micro', 'samples', 'weighted', 'macro'} or None, \\\n",
    "            default='macro'\n",
    "        If ``None``, the scores for each class are returned. Otherwise,\n",
    "        this determines the type of averaging performed on the data:\n",
    "        ``'micro'``:\n",
    "            Calculate metrics globally by considering each element of the label\n",
    "            indicator matrix as a label.\n",
    "        ``'macro'``:\n",
    "            Calculate metrics for each label, and find their unweighted\n",
    "            mean.  This does not take label imbalance into account.\n",
    "        ``'weighted'``:\n",
    "            Calculate metrics for each label, and find their average, weighted\n",
    "            by support (the number of true instances for each label).\n",
    "        ``'samples'``:\n",
    "            Calculate metrics for each instance, and find their average.\n",
    "        Will be ignored when ``y_true`` is binary.\n",
    "\n",
    "    pos_label : int or str, default=1\n",
    "        The label of the positive class. Only applied to binary ``y_true``.\n",
    "        For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.\n",
    "        \n",
    "    sample_weight : array-like of shape (n_samples,), default=None\n",
    "        Sample weights.\n",
    "\n",
    "    Returns:\n",
    "        auc : float \n",
    "            Area under ROC for predicted classes\n",
    "    \"\"\"\n",
    "    if transform:\n",
    "        return sklm_to_scorer(average_precision_score, average=average, sample_weight=sample_weight, **kwargs)\n",
    "\n",
    "    return partial(average_precision_score,average=average, sample_weight=sample_weight, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from sklearn.datasets import make_multilabel_classification, make_classification, make_regression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "from sklearn.multioutput import ClassifierChain, RegressorChain\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d8c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=False\n",
    "iterators = {'regression': {'R2': R2Score(transform=transform),\n",
    "                            'RMSE': MSEScore(transform=transform,squared=False),\n",
    "                            'MSE': MSEScore(transform=transform,squared=True),\n",
    "                            'MAE': MAEScore(transform=transform)},\n",
    "             'classification': {'MCC': MatthewsCorrCoef(transform=transform),\n",
    "                                'Se': RecallScore(transform=transform, pos_label=1),\n",
    "                                'Sp': RecallScore(transform=transform, pos_label=0),\n",
    "                                'Precision': PrecisionScore(transform=transform, pos_label=1),\n",
    "                                'PR-AUC': APScore(transform=transform),\n",
    "                                'ROC-AUC': ROCAucScore(transform=transform)},\n",
    "             'multi-class': {'MCC-Multi': MatthewsCorrCoef(transform=transform),\n",
    "                             'Se-Multi': RecallScore(transform=transform, average='macro'),\n",
    "                             'Sp-Multi': RecallScore(transform=transform, average='macro'),\n",
    "                             'Precision-Multi': PrecisionScore(transform=transform, average='macro'),\n",
    "                             'PR-AUC-Multi': APScore(transform=transform, average='macro'),\n",
    "                             'ROC-AUC-Multi': ROCAucScore(transform=transform, average='macro')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_multi_reg, y_multi_reg = make_regression(n_samples=200, n_features=20, n_targets=5)\n",
    "\n",
    "X_multi_class, y_multi_class = make_classification(n_samples=200, n_features=20, n_informative=15, n_redundant=5, n_classes=3, n_clusters_per_class=6)\n",
    "y_multi_class_bin = label_binarize(y_multi_class, classes=np.unique(y_multi_class))\n",
    "X_multi_label, y_multi_label = make_multilabel_classification(n_samples=200, n_features=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4766f",
   "metadata": {},
   "source": [
    "**Multiregression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X_multi_reg, y_multi_reg,test_size=0.2)\n",
    "model.fit(Xtrain, ytrain)\n",
    "\n",
    "preds = model.predict(Xtest)\n",
    "\n",
    "iterators['regression']['R2'](ytest, preds), iterators['regression']['RMSE'](ytest, preds), iterators['regression']['MSE'](ytest, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()#RegressorChain(RandomForestRegressor())\n",
    "cv_k = cross_validate(model, X_multi_reg, y_multi_reg, cv=5, scoring={'MAE':MAEScore(transform=True),\n",
    "                                                                     'RMSE':MSEScore(transform=True,squared=False),\n",
    "                                                                     'MSE':MSEScore(transform=True,squared=False),\n",
    "                                                                     'R2':R2Score(transform=True)})\n",
    "cv_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c797a51",
   "metadata": {},
   "source": [
    "**Multiclass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc36a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "cv_k = cross_validate(model, X_multi_class, y_multi_class, cv=5, scoring={'MCC':MatthewsCorrCoef(transform=True),\n",
    "                                                                         'Fbeta':FBetaScore(transform=True,beta=0.5,average='macro')})\n",
    "cv_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2f653",
   "metadata": {},
   "source": [
    "**Multilabel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "model = ClassifierChain(RandomForestClassifier())\n",
    "cv_k = cross_validate(model, X_multi_label, y_multi_label, cv=5, scoring={'MCC':MatthewsCorrCoef(transform=True)})\n",
    "cv_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45018f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "model.fit(X_multi_label,y_multi_label)\n",
    "probas = model.predict(X_multi_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "FBetaScore()(y_multi_label, probas, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "RecallScore(y_multi_label, probas, average='macro', transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398150fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# APScore\n",
    "g2 = APScore(y_true, y_scores, transform=False,average='weighted')\n",
    "g1 = average_precision_score(y_true, y_scores)\n",
    "assert g1 == g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32aed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# ROCAUC\n",
    "g2 = ROCAucScore(y_true, y_scores, transform=False,average='macro')\n",
    "g1 = roc_auc_score(y_true, y_scores)\n",
    "assert g1 == g2\n",
    "print(g1, g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04171fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Recall\n",
    "avg = 'binary'\n",
    "g2 = RecallScore(y_true, np.where(y_scores>=0.5,1,0), transform=False,average=avg)\n",
    "g1 = recall_score(y_true, np.where(y_scores>=0.5,1,0),average=avg)\n",
    "print(g1, g2)\n",
    "assert g1 == g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Precision\n",
    "avg = 'micro'\n",
    "g2 = PrecisionScore(y_true, np.where(y_scores>=0.5,1,0), transform=False,average=avg)\n",
    "g1 = precision_score(y_true, np.where(y_scores>=0.5,1,0),average=avg)\n",
    "print(g1, g2)\n",
    "assert g1 == g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82591519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Accuracy\n",
    "avg = 'micro'\n",
    "g2 = BalancedAccuracyScore(y_true, np.where(y_scores>=0.5,1,0), transform=False)\n",
    "g1 = balanced_accuracy_score(y_true, np.where(y_scores>=0.5,1,0))\n",
    "print(g1, g2)\n",
    "assert g1 == g2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
