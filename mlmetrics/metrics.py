# AUTOGENERATED! DO NOT EDIT! File to edit: metrics.ipynb (unless otherwise specified).

__all__ = ['sklm_to_scorer', 'R2Score', 'MSEScore', 'MAEScore', 'BalancedAccuracyScore', 'MatthewsCorrCoef',
           'PrecisionScore', 'RecallScore', 'FBetaScore', 'ROCAucScore', 'APScore']

# Cell
import numpy as np
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer, fbeta_score, f1_score, roc_auc_score, precision_score, recall_score, average_precision_score, balanced_accuracy_score, matthews_corrcoef
from functools import partial

# Cell
def sklm_to_scorer(func, transform:bool=False, **kwargs):
    """Converts func into sklearn scorer"""

    return make_scorer(func, **kwargs)


def R2Score(y_true=None, y_pred=None, sample_weight=None, multioutput:str='uniform_average', transform:bool=False, **kwargs):

    """
    :math:`R^2` (coefficient of determination) regression score function.

    Arguments

     y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
        Ground truth (correct) target values.

    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
        Estimated target values.

    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.

    multioutput : {'raw_values', 'uniform_average', 'variance_weighted'}, \
            array-like of shape (n_outputs,) or None, default='uniform_average'
        Defines aggregating of multiple output scores.
        Array-like value defines weights used to average scores.
        Default is "uniform_average".
        'raw_values' :
            Returns a full set of scores in case of multioutput input.
        'uniform_average' :
            Scores of all outputs are averaged with uniform weight.
        'variance_weighted' :
            Scores of all outputs are averaged, weighted by the variances
            of each individual output.
        .. versionchanged:: 0.19
            Default value of multioutput is 'uniform_average'.

    Returns:
        r2 : float
            The  score :math:`R^2` or ndarray of scores if ‘multioutput’ is ‘raw_values’.
    """
    if transform:
        return sklm_to_scorer(r2_score, sample_weight=sample_weight, multioutput=multioutput, **kwargs)

    return partial(r2_score, sample_weight=sample_weight, multioutput=multioutput, **kwargs)#(y_true=y_true, y_pred=y_pred, sample_weight=sample_weight, multioutput=multioutput, **kwargs)



def MSEScore(y_true=None, y_pred=None, sample_weight=None, multioutput:str='uniform_average', squared=True, transform:bool=False, **kwargs):

    """
    Mean squared error regression loss.

    Arguments

    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
        Ground truth (correct) target values.

    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
        Estimated target values.

    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.

    multioutput : {'raw_values', 'uniform_average'} or array-like of shape \
            (n_outputs,), default='uniform_average'
        Defines aggregating of multiple output values.
        Array-like value defines weights used to average errors.
        'raw_values' :
            Returns a full set of errors in case of multioutput input.
        'uniform_average' :
            Errors of all outputs are averaged with uniform weight.

    Returns:
        mse : float
             If True returns MSE value, if False returns RMSE value.
    """
    if transform:
        return sklm_to_scorer(mean_squared_error, sample_weight=sample_weight, multioutput=multioutput, squared=squared, **kwargs)

    return partial(mean_squared_error, sample_weight=sample_weight, multioutput=multioutput, squared=squared, **kwargs)


def MAEScore(y_true=None, y_pred=None, sample_weight=None, multioutput:str='uniform_average', transform:bool=False, **kwargs):

    """
    Mean absolute error regression loss.

    Arguments

    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
        Ground truth (correct) target values.

    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
        Estimated target values.

    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.

    multioutput : {'raw_values', 'uniform_average'} or array-like of shape \
            (n_outputs,), default='uniform_average'
        Defines aggregating of multiple output values.
        Array-like value defines weights used to average errors.
        'raw_values' :
            Returns a full set of errors in case of multioutput input.
        'uniform_average' :
            Errors of all outputs are averaged with uniform weight.

    Returns:
        mae : float
             returns MAE.
    """
    if transform:
        return sklm_to_scorer(mean_absolute_error, sample_weight=sample_weight, multioutput=multioutput, **kwargs)

    return partial(mean_absolute_error, sample_weight=sample_weight, multioutput=multioutput, **kwargs)


def BalancedAccuracyScore(y_true=None, y_pred=None, sample_weight=None, adjusted:bool=False, transform:bool=False, **kwargs):

    """
    Computes Matthew's correlation coefficient between actual and predicted values.
    Arguments

        y_true : array-like
            Ground truth (correct) target values.

        y_pred : array-like
            Estimated targets as returned by a classifier.

        sample_weight : array-like, default=None
            Sample weights.

        adjusted : bool, default=False
            When true, the result is adjusted for chance, so that random performance would score 0, while keeping perfect performance at a score of 1.


    Returns:
        balanced_accuracy : float
            Balanced accuracy score.
    """
    if transform:
        return sklm_to_scorer(balanced_accuracy_score, sample_weight=sample_weight, adjusted=adjusted, **kwargs)

    return partial(balanced_accuracy_score, sample_weight=sample_weight, adjusted=adjusted, **kwargs)


def MatthewsCorrCoef(y_true=None, y_pred=None, sample_weight=None, transform:bool=False, **kwargs):

    """
    Computes Matthew's correlation coefficient between actual and predicted values.
    Arguments

        y_true : array-like
            Ground truth (correct) target values.

        y_pred : array-like
            Estimated targets as returned by a classifier.

        sample_weight : array-like
            Sample weights.


    Returns:
        mcc : float
        The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction).
    """
    if transform:
        return sklm_to_scorer(matthews_corrcoef, sample_weight=sample_weight, **kwargs)

    return partial(matthews_corrcoef, sample_weight=sample_weight, **kwargs)


def PrecisionScore(y_true=None, y_pred=None, average:str='binary', pos_label:int=1, sample_weight=None, transform:bool=False,**kwargs):


    """
    Computes Precision between actual and predicted values
    Arguments

        y_true : array-like
            Ground truth (correct) target values.

        y_pred : array-like
            Estimated targets as returned by a classifier.


        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, default='binary'

            This parameter is required for multiclass/multilabel targets.
            If ``None``, the scores for each class are returned. Otherwise, this
            determines the type of averaging performed on the data:
            ``'binary'``:
                Only report results for the class specified by ``pos_label``.
                This is applicable only if targets (``y_{true,pred}``) are binary.
            ``'micro'``:
                Calculate metrics globally by counting the total true positives,
                false negatives and false positives.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average weighted
                by support (the number of true instances for each label). This
                alters 'macro' to account for label imbalance; it can result in an
                F-score that is not between precision and recall.
            ``'samples'``:
                Calculate metrics for each instance, and find their average (only
                meaningful for multilabel classification where this differs from
                :func:`accuracy_score`).

        sample_weight : array-like
            Sample weights.


    Returns:
        precision : float (if average is not None) or array of float of shape (n_unique_labels,)
            Precision of the positive class in binary classification or weighted
            average of the precision of each class for the multiclass task.
    """
    if transform:
        return sklm_to_scorer(precision_score, average=average, pos_label=pos_label, sample_weight=sample_weight, **kwargs)

    return partial(precision_score, average=average, pos_label=pos_label, sample_weight=sample_weight, **kwargs)


def RecallScore(y_true=None, y_pred=None, average:str='binary', pos_label:int=1, sample_weight=None,
                transform:bool=False, **kwargs):

    """
    Computes Recall (Senstivity) between actual and predicted values
    Arguments

        y_true : array-like
            Ground truth (correct) target values.

        y_pred : array-like
            Estimated targets as returned by a classifier.


        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, default='binary'

            This parameter is required for multiclass/multilabel targets.
            If ``None``, the scores for each class are returned. Otherwise, this
            determines the type of averaging performed on the data:
            ``'binary'``:
                Only report results for the class specified by ``pos_label``.
                This is applicable only if targets (``y_{true,pred}``) are binary.
            ``'micro'``:
                Calculate metrics globally by counting the total true positives,
                false negatives and false positives.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average weighted
                by support (the number of true instances for each label). This
                alters 'macro' to account for label imbalance; it can result in an
                F-score that is not between precision and recall.
            ``'samples'``:
                Calculate metrics for each instance, and find their average (only
                meaningful for multilabel classification where this differs from
                :func:`accuracy_score`).

        sample_weight : array-like
            Sample weights.


    Returns:
        recall : float (if average is not None) or array of float of shape (n_unique_labels,)
            Recall of the positive class in binary classification or weighted
            average of the recall of each class for the multiclass task.
    """

    if transform:
        return sklm_to_scorer(recall_score, average=average, pos_label=pos_label, sample_weight=sample_weight, **kwargs)

    return partial(recall_score, average=average, pos_label=pos_label, sample_weight=sample_weight, **kwargs)

def FBetaScore(y_true=None, y_pred=None, beta:float=0.5, pos_label:int=1, average:str='binary', sample_weight=None,transform:bool=False,**kwargs):

    """
    Compute the F-beta score between actual and predicted values

        Arguments

    y_true : array-like
        Ground truth (correct) target values.

    y_pred : array-like
        Estimated targets as returned by a classifier.


    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, default='binary'

        This parameter is required for multiclass/multilabel targets.
        If ``None``, the scores for each class are returned. Otherwise, this
        determines the type of averaging performed on the data:
        ``'binary'``:
        Only report results for the class specified by ``pos_label``.
        This is applicable only if targets (``y_{true,pred}``) are binary.
        ``'micro'``:
        Calculate metrics globally by counting the total true positives,
        false negatives and false positives.
        ``'macro'``:
        Calculate metrics for each label, and find their unweighted
        mean.  This does not take label imbalance into account.
        ``'weighted'``:
        Calculate metrics for each label, and find their average weighted
        by support (the number of true instances for each label). This
        alters 'macro' to account for label imbalance; it can result in an
        F-score that is not between precision and recall.
        ``'samples'``:
        Calculate metrics for each instance, and find their average (only
        meaningful for multilabel classification where this differs from
        :func:`accuracy_score`).

    sample_weight : array-like
        Sample weights.


    Returns:
        fbeta_score : float (if average is not None) or array of float, shape = [n_unique_labels]
        F-beta score of the positive class in binary classification or weighted average of the F-beta score of each class for the multiclass task.
    """
    if transform:
        return sklm_to_scorer(fbeta_score, beta=beta, average=average, pos_label=pos_label, sample_weight=sample_weight, **kwargs)

    return partial(fbeta_score, beta=beta, average=average, pos_label=pos_label, sample_weight=sample_weight, **kwargs)

def ROCAucScore(y_true=None, y_score=None, average:str='macro', sample_weight=None, transform:bool=False, **kwargs):

    """
    Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.
    Arguments

    y_true : array-like
        Ground truth (correct) target values.


    y_score : array-like

        Target scores.
        * In the binary case, it corresponds to an array of shape
          `(n_samples,)`. Both probability estimates and non-thresholded
          decision values can be provided. The probability estimates correspond
          to the **probability of the class with the greater label**,
          i.e. `estimator.classes_[1]` and thus
          `estimator.predict_proba(X, y)[:, 1]`. The decision values
          corresponds to the output of `estimator.decision_function(X, y)`.
          See more information in the :ref:`User guide <roc_auc_binary>`;
        * In the multiclass case, it corresponds to an array of shape
          `(n_samples, n_classes)` of probability estimates provided by the
          `predict_proba` method. The probability estimates **must**
          sum to 1 across the possible classes. In addition, the order of the
          class scores must correspond to the order of ``labels``,
          if provided, or else to the numerical or lexicographical order of
          the labels in ``y_true``. See more information in the
          :ref:`User guide <roc_auc_multiclass>`;
        * In the multilabel case, it corresponds to an array of shape
          `(n_samples, n_classes)`. Probability estimates are provided by the
          `predict_proba` method and the non-thresholded decision values by
          the `decision_function` method. The probability estimates correspond
          to the **probability of the class with the greater label for each
          output** of the classifier. See more information in the
          :ref:`User guide <roc_auc_multilabel>`.


    average : {'micro', 'macro', 'samples', 'weighted'} or None, \
            default='macro'
        If ``None``, the scores for each class are returned. Otherwise,
        this determines the type of averaging performed on the data:
        Note: multiclass ROC AUC currently only handles the 'macro' and
        'weighted' averages.
        ``'micro'``:
            Calculate metrics globally by considering each element of the label
            indicator matrix as a label.
        ``'macro'``:
            Calculate metrics for each label, and find their unweighted
            mean.  This does not take label imbalance into account.
        ``'weighted'``:
            Calculate metrics for each label, and find their average, weighted
            by support (the number of true instances for each label).
        ``'samples'``:
            Calculate metrics for each instance, and find their average.
        Will be ignored when ``y_true`` is binary.

    sample_weight : array-like
        Sample weights.


    Returns:
        auc : float
            Area under ROC for predicted classes
    """
    if transform:
        return sklm_to_scorer(roc_auc_score, average=average, sample_weight=sample_weight, **kwargs)

    return partial(roc_auc_score, average=average, sample_weight=sample_weight, **kwargs)



def APScore(y_true=None, y_score=None, average:str='macro', sample_weight=None, transform:bool=False, **kwargs):

    """
    Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.
    Arguments

    y_true : array-like
        Ground truth (correct) target values.


    y_score : array-like

        Target scores, can either be probability estimates of the positive
        class, confidence values, or non-thresholded measure of decisions
        (as returned by :term:`decision_function` on some classifiers).


    average : {'micro', 'samples', 'weighted', 'macro'} or None, \
            default='macro'
        If ``None``, the scores for each class are returned. Otherwise,
        this determines the type of averaging performed on the data:
        ``'micro'``:
            Calculate metrics globally by considering each element of the label
            indicator matrix as a label.
        ``'macro'``:
            Calculate metrics for each label, and find their unweighted
            mean.  This does not take label imbalance into account.
        ``'weighted'``:
            Calculate metrics for each label, and find their average, weighted
            by support (the number of true instances for each label).
        ``'samples'``:
            Calculate metrics for each instance, and find their average.
        Will be ignored when ``y_true`` is binary.

    pos_label : int or str, default=1
        The label of the positive class. Only applied to binary ``y_true``.
        For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.

    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.

    Returns:
        auc : float
            Area under ROC for predicted classes
    """
    if transform:
        return sklm_to_scorer(average_precision_score, average=average, sample_weight=sample_weight, **kwargs)

    return partial(average_precision_score,average=average, sample_weight=sample_weight, **kwargs)